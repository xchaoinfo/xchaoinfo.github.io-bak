[{"title":"Python3 读写 csv","date":"2017-04-13T14:46:42.461Z","path":"2017/04/13/04-- Python3 读写 csv /","text":"Python3 读写 csv简介xchaoinfo 在用 Python 写爬虫的时候，百万以内的数据量，经常使用 csv 这种格式来存放数据。优点： Python 内置模块读写方便 纯文本格式并且保持一定的数据结构 可以被 Excel 打开 以下是 Python3 下读写 csv 实例访问 https://github.com/xchaoinfo/Py-example-by-xchaoinfo获取详细的代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#!/usr/bin/env python3# -*- coding: utf-8 -*-# @Date : 2017-02-26 22:59:55# @Author : xchaoinfo (xchaoinfo)# @github : https://github.com/xchaoinfoimport csv&quot;&quot;&quot;data1.csv 的内容是张三,北京,25李四,上海,30data2.csv 的内容是姓名,城市,年龄张三,北京,25李四,上海,30&quot;&quot;&quot;# example 1 读取 csv 返回列表def read_csv(fn=&apos;data1.csv&apos;): &quot;&quot;&quot; data1.csv 中数据格式， 读取返回列表比较容易处理 &quot;&quot;&quot; # 读取打印 - reader-example1 with open(fn, encoding=&quot;utf-8&quot;) as fr: csvfr = csv.reader(fr) for row in csvfr: print(&quot;, &quot;.join(row)) # 读取后，返回列表 - reader-example with open(fn, encoding=&quot;utf-8&quot;) as fr: csvfr = csv.reader(fr) rows = [row for row in csvfr] return rows# example 2 读取 csv 返回列表def read_csv_dict(fn=&quot;data2.csv&quot;): &quot;&quot;&quot; data2.csv 的数据格式， 读取返回字典比较容易处理 &quot;&quot;&quot; with open(fn, encoding=&quot;utf-8&quot;) as fr: dict_rows = csv.DictReader(fr, fieldnames=None) # 不指定 fieldnames 的情况下 # 默认第一行数据作为 fieldnames for row in dict_rows: print(row) # row 是一个字典# example 3 list 或 tuple 写入到 csvdef write_csv(fn=&quot;data1.csv&quot;): with open(fn, &apos;w&apos;, newline=&quot;&quot;, encoding=&quot;utf-8&quot;) as fw: fwcsv = csv.writer(fw) one = (&quot;张三&quot;, &quot;北京&quot;, &quot;25&quot;) # tuple two = [&quot;李四&quot;, &quot;上海&quot;, &quot;30&quot;] # list fwcsv.writerow(one) fwcsv.writerow(two)# example 4 dict 写入到 csv 中def write_dict_csv(fn=&quot;data2.csv&quot;): dict1 = &#123; &quot;姓名&quot;: &quot;张三&quot;, &quot;城市&quot;: &quot;北京&quot;, &quot;年龄&quot;: &quot;25&quot; &#125; dict2 = &#123; &quot;姓名&quot;: &quot;李四&quot;, &quot;城市&quot;: &quot;上海&quot;, &quot;年龄&quot;: &quot;30&quot; &#125; fieldnames = [&quot;姓名&quot;, &quot;城市&quot;, &quot;年龄&quot;] with open(fn, &apos;w&apos;, newline=&quot;&quot;, encoding=&quot;utf-8&quot;) as fwd: fwdcsv = csv.DictWriter(fwd, fieldnames=fieldnames) fwdcsv.writeheader() fwdcsv.writerow(dict1) fwdcsv.writerow(dict2)if __name__ == &apos;__main__&apos;: write_dict_csv() write_csv() read_csv() read_csv_dict()","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"CSV","slug":"CSV","permalink":"http://yoursite.com/tags/CSV/"}]},{"title":"xlwings 让你的 Excel 飞起来 ","date":"2017-03-16T08:39:12.000Z","path":"2017/03/16/05-- xlwings 让你的 Excel 飞起来/","text":"xlwings 让你的 Excel 飞起来简介众所周知，VBA 可以很高效的操作 Excel，提高办公效率。在 Python 中，我们可以通过 pywin32 来调用 windows 系统的 API 来实现 VBA 的很多功能，但是写起来比较复杂。 开始之前, 回忆一下使用 Excel 的场景。我们会同时打开多个 Excel 文件, 多个 workbook, 每个 workbook 又可以用多个 sheet。同时，我们还会在多个 sheet workbook Excel 窗口之间切换。对一个或多个单元格进行增删改查，设置格式，合并分拆等操作。利用 xlwings 这些操作都可以轻松搞定，一次编写彻底解决”手抽筋的问题” xlwings 是基于 BSD-licensed 的一个 Python 第三方的模块，对 pywin32 进行了封装，可以很方便的和 Excel 进行交互，它有以下优点： 语法接近 VBA 可以用 Python 代码取代 VBA 编写宏 在 windows 可以用 Python 编写 Excel 用户自定义函数 全功能支持 Numpy Pandas matplotlib 等科学计算库 支持 Windows 和 MacOS 支持 Py2.7 Py3.3+ xlwings 可以让你的 Excel 飞起来，正如 Selenium 可以让你的浏览器飞起来一样。 安装推荐使用 Anaconda https://www.continuum.io/downloads 来安装，可以省去很多麻烦，install xlwings```123456使用 pip 安装，需要先手动安装 pywin32 下载地址https://sourceforge.net/projects/pywin32/files/pywin32/ 安装 pywin32 后，使用 pip 安装即可```pip install xlwings 官方安装文档http://docs.xlwings.org/en/stable/installation.html 快速入门打开工作表1234567891011import xlwings as xw# 打开一个新的 workbook wb = xw.Book()# 打开当前目录已经存在的一个 workbook wb = xw.Book(&apos;FileName.xlsx&apos;)# 输入完整的路径打开一个 workbook FileName = &quot;C:\\\\python\\\\to\\\\file.xlsx&quot;FileName = r&quot;C:\\python\\to\\file.xlsx&quot;# 注意 windows 字符 &quot;\\&quot; 逃逸的问题wb = xw.Book(fn) 打开 sheet 的三种方式1234567# 打开第一个 sheetsheet = wb.sheets[0]# 打开名字为 &quot;xchaoinfo&quot; sheetsheet = wb.sheets[&quot;xchaoinfo&quot;]# 打开当前活动的 sheetsheet = wb.sheets.active 读写数据到 sheet 中1234567891011121314151617181920212223242526272829303132# 当前活动的 sheet 中读写一个单元格的数据&gt;&gt;&gt; import xlwings as xw&gt;&gt;&gt; wb = xw.Book()&gt;&gt;&gt; sht = wb.sheets.active&gt;&gt;&gt; sht.range(&apos;A1&apos;).value = &quot;xchaoinfo&quot;&gt;&gt;&gt; sht.range(&apos;A1&apos;).value&apos;xchaoinfo&apos;# 当前活动的 sheet 中读写一行单元格的数据# 将列表储存在A1：C1中&gt;&gt;&gt; sht.range(&apos;A1&apos;).value=[&quot;name&quot;,&quot;age&quot;,&quot;gender&quot;]&gt;&gt;&gt; sht.range(&apos;A1:C1&apos;).value[&apos;name&apos;, &apos;age&apos;, &apos;gender&apos;]# 当前活动的 sheet 中读写一列单元格的数据# 将列表储存在A1:A3中&gt;&gt;&gt; sht.range(&apos;A1&apos;).options(transpose=True).value=[&quot;xchaoinfo&quot;,18,1]&gt;&gt;&gt; sht.range(&quot;A1:A3&quot;).value[&apos;xchaoinfo&apos;, 18.0, 1.0]&gt;&gt;&gt; sht.range(&apos;A1:A3&apos;).value = [&quot;Robot&quot;, 20, 2]&gt;&gt;&gt; sht.range(&quot;A1:A3&quot;).value[&apos;Robot&apos;, 18.0, 1.0]# 当前活动的 sheet 中读写多行多列单元格的数据# 将2x2表格，即二维数组，储存在A1:B2中，如第一行1，2，第二行3，4&gt;&gt;&gt; wb = xw.Book()&gt;&gt;&gt; sht = wb.sheets.active&gt;&gt;&gt; sht.range(&apos;A1&apos;).options(expand=&apos;table&apos;)=[[1,2],[3,4]]&gt;&gt;&gt; sht.range(&quot;A1&quot;).expand().value[[1.0, 2.0], [3.0, 4.0]]# expand 的详细用法请参考文档&gt;&gt;&gt; sht.range(&apos;A1:B2&apos;).value = [[1,2],[3,4]]&gt;&gt;&gt; sht.range(&apos;A1:B2&apos;).value[[1.0, 2.0], [3.0, 4.0]] 应用实例删除 Excel 文件中，满足条件的单元格所在的一整行 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#!/usr/bin/env python3# -*- coding: utf-8 -*-# @Date : 2017-03-16 14:14:03# @Author : xchaoinfo (xchaoinfo)# @github : https://github.com/xchaoinfoimport xlwings as xwfn = &quot;data.xlsx&quot;class DeleteTools(object): &quot;&quot;&quot;删除满足某些条件的行 data.xlsx 中有很多重复的数据 需要删除那些重复的 &quot;&quot;&quot; def __init__(self, fn): super(DeleteTools, self).__init__() self.ExistSet = set() self.ToDelList = list() self.fn = fn def rule(self, value): # 可以自定义规则来操作 pass def Delete(self): # visible 控制 Excel 打开是否显示界面 # add_book 控制是否添加新的 workbook app = xw.App(visible=True, add_book=False) # app.display_alerts = False # 打开 data.xlsx 文件到 wookbook 中 wb = app.books.open(fn) # 切换到当前活动的 sheet 中 sheet = wb.sheets.active # 选择 A1 所在的一列 # 当 Excel 格式复杂的时候,不建议使用 expand # 可以这样选择 ARange = sheet.range(&quot;A1:A100&quot;) # ARange = sheet.range(&quot;A1&quot;).expand(&quot;down&quot;) for A in ARange: if str(A.value).strip() not in self.ExistSet: self.ExistSet.add(str(A.value).strip()) else: # address = A.address # 获取 A 所在的位置坐标 self.ToDelList.append(A.address) # print(A.value) while self.ToDelList: td = self.ToDelList.pop() # 删除 A 所在的一行 sheet.range(td).api.EntireRow.Delete() # 保存 wookbook # 相当于Excel 的 Ctrl+S 快捷键 sheet.autofit() wb.save() app.quit()if __name__ == &apos;__main__&apos;: d = DeleteTools(fn) d.Delete() 参考插上翅膀，让Excel飞起来——xlwings（一）http://www.jianshu.com/p/e21894fc5501xlwings 官方文档http://docs.xlwings.org/en/stable/ 更多内容详见官方文档 文中的代码示例可以在 github 上找到 https://github.com/xchaoinfo/Py-example-by-xchaoinfo本文同步发布在 知乎专栏: 可爱的 Python 首发于微信公众号：xchaoinfo","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"Python 构建一个简单爬虫系统 (二)","date":"2016-10-01T13:43:37.000Z","path":"2016/10/01/02-- Python 构建一个简单爬虫系统 二/","text":"Python 构建一个简单爬虫系统 (二)本文是用 Python 构建一个简单爬虫系统的第二篇，上一篇介绍了通过 requests 和 Beautifulsoup 来做一个网页的抓取和解析。本篇介绍通过 queue 和 threading 模块，使用队列和多线程来进行大规模数据的抓取。 目录 0x00·背景简介 0x01·queue、threading 基础 0x02·多线程爬虫实例 背景简介Q1: 据说由于 GIL(全局锁) 的存在，Python 多线程很鸡肋，多线程 Python 爬虫能提高速度吗？ A1: 要很好的回答这个问题，首先要搞清楚两个概念：进程与线程，I/O 密集型与计算密集型任务。 进程与线程操作系统的设计，可以归结为三点： 以多进程形式，允许多个任务同时运行； 以多线程形式，允许单个任务分成不同的部分运行； 提供协调机制，一方面防止进程之间和线程之间产生冲突，另一方面允许进程之间和线程之间共享资源。 I/O 密集型与计算密集型任务简单的来说是频繁的进行 CPU 的计算，还是文件的读写操作。 总结对于计算密集型的任务，由于 GIL 的存在使用多进程，能提高计算效率对于I/O 密集型的任务，例如网络爬虫，大多数时间都是等待从网上下载文件写入到本地，使用多线程能显著提高爬虫的爬取速度。 queue、threading 基础queuequeue 模块提供了一个适用于多线程编程的数据结构，可以用来在线程间安全地传递消息或者其他数据，它会为提供者处理锁定，使多个线程可以安全地处理同一个 queue 实例。 下面介绍一些简单的用法，更复杂的用法参考 queue 的文档 12345678910111213141516# 创建一个 queue 实例q = queue.Queue()# 向 queue 添加一个元素item = \"xchaoinfo\"q.put(item)# 从 queue 取得一个元素data = q.get()# 获取 queue 的元素的个数q.qsize()# 判断 queue 是否为空q.empty() threadingthreading 在 _thread 模块的基础的提供多线程处理的更高级别的接口。 下面介绍一些 threading 的简单用法 不带参数的线程创建1234567import threadingdef do_work(): print(\"start working\")t = threading.Thread(target=do_work)t.start() 带参数的线程创建12345678import threadingdef do_work(num): print(\"start working of %s\" % num)# args 是 tuple 类型的参数，t = threading.Thread(target=do_work, args=(1,))t.start() 其实，更常用的方法是继承 threading.Thread 类，然后重写 run 方法 多线程爬虫实例假设我们已经把要爬取的 url 存放到 url.txt 文件里，并且每行存放一个 url, 每个 url 都是一张图片的地址，我们要把所有图片下载到本地，把 url 的后面部分来作为图片的文件名123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import requestsimport threadingimport queueimport os# 从 txt 文件中获取 url, 并且处理下载后的文件名def get_url_fn_from_txt(): path = \"img/\" if not os.path.exists(path): os.mkdir(path) with open(\"url.txt\") as fr: url_list = [f.strip() for f in fr if f.strip()] url_fn_list = [tuple(u, path + u.split(\"/\")[-1]) for u in url_list] return url_fn_list# 下载 img 的函数，下载成功返回 1 否则返回 0def download_img(url, fn): try: if os.path.exists(fn): return 1 img = requests.get(url, timeout=3) with open(fn, 'wb') as fw: fw.write(img.content) fw.close() return 1 except: return 0class DownloadThread(threading.Thread): \"\"\"重写 run 函数\"\"\" def __init__(self, que): self.que = que def run(self): while not self.que.empty(): url, fn = self.que.get() # for u in uid: if download_img(url, fn): pass else: self.que.put(tuple(url, fn))def main(): url_fn_list = get_url_fn_from_txt() que = queue.Queue() for url_fn in url_fn_list: que.put(url_fn) thread_num = 20 # 线程数 for i in range(thread_num): download = DownloadThread(que) download.start()if __name__ == '__main__': main() 这是比较简陋的一个实例，可以在此基础上加入更多功能。如有错误，烦请指出，不胜感激。 参考： http://www.ruanyifeng.com/blog/2013/04/processes_and_threads.html (进程与线程) http://blog.chinaunix.net/uid-116213-id-3086203.html (I/O 密集型与计算密集型任务) queue 和 threading 的用法参考 Python 的文档","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"Python 构建一个简单爬虫系统 (一)","date":"2016-09-30T09:37:28.000Z","path":"2016/09/30/01-- Python 构建一个简单爬虫系统 一/","text":"Python 构建一个简单爬虫系统 (一)本文通过 requests beautifulsoup re 等 Python的模块, 尝试构建一个微型的爬虫系统，本文采用 Python 3 的版本, 本文是第一篇介绍一个网页的简单抓取和解析 目录 0x00·简介 0x01·简单抓取 0x02·解析网页 0x00 简介 requests 作为优秀的 Python 第三方的模块，当邂逅了之后，你就会向 urllib say goodbye 了, 本文会介绍一些 requests 的具体的使用方法。 beautifulsoup 作为常用的解析网页的第三方的模块，通过 “lxml” 作为解析器，可以达到速度和兼容性的平衡.当然 xpath 也是一个很好的选择。re 还是无处不在的显示其强大的生命力。 在了解 requests Beautifulsoup4 的用法的基础上，一步步教你怎么构建一个简单的小型爬虫系统， 本文是对 requests Beautifulsoup4 的一个简单的综合应用，关于这两 Python 优秀的第三方模块的使用方法，可以看本文最后我给的参考文档 0x01 简单抓取抓取一个网页并且把内容保存到本地 import requests def get_html(url): html = requests.get(url) with open(&apos;content.html&apos;, &apos;wb&apos;) as fw: fw.write(html.content) fw.close() if __name__ == &apos;__main__&apos;: url = &quot;http://example.com&quot; get_data(url) 可是你会发现，大多数情况，这个代码是没法工作的，网站会对 request header 做一些限制， def get_html(url): headers = { &apos;User-Agent&apos;: &quot;Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.87 Safari/537.36&quot; } html = requests.get(url, headers=headers) with open(&apos;content.html&apos;, &apos;wb&apos;) as fw: fw.write(html.content) fw.close() if __name__ == &apos;__main__&apos;: url = &quot;http://example.com&quot; get_html(url) 主要是后面的方法，我们假装好像浏览器的样子，还有些网站需要检测 Request headers 里面的 Refer host 等字段，我们会在后面讲到但是 0x01-1 的代码，还是不够健壮，因为，我们不知道请求是否成功了，如果最后保存到本地的，是一堆 404 的页面，显然是一件很让人沮丧的事情，我们在把代码改下，加上一个请求是否成功的代码。 http 请求状态有很多种，大多数情况下 200 是我们理想中的状态， def get_data(url): headers = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.87 Safari/537.36&quot; } html = requests.get(url, headers=headers) if html.status_code == 200: print(url, &apos;@ok200&apos;, str(time.ctime())) with open(&apos;content.html&apos;, &apos;wb&apos;) as fw: fw.write(html.content) fw.close() else: print(url, &apos;wrong&apos;, str(time.ctime())) if __name__ == &apos;__main__&apos;: url = &quot;http://example.com&quot; get_data(url) 我们用 requests 的一个 status_code 方法来判断是否访问成功，然后使用 print 一下，相当于打印日志了，当我们做大规模抓取的时候，print 显然不是一个很好的选择，Python 的 logging 模块能够更好的胜任这项工作，为了方便起见，我们暂时使用简单直接粗暴的 print 来做代替 logging 的工作 由于网络原因，可能会出现访问超时的问题，为了让代码更健壮，我们可以设置 timeout, 同时，如果确定网页不需要重定向， 可以设置 allow_redirects=False def get_data(url): headers = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.87 Safari/537.36&quot; } try: html = requests.get(url, headers=headers, allow_redirects=Fasle, timeout=3) if html.status_code == 200: print(url, &apos;@ok200&apos;, str(time.ctime())) with open(&apos;content.html&apos;, &apos;wb&apos;) as fw: fw.write(html.content) fw.close() else: print(url, &apos;wrong&apos;, str(time.ctime())) except Exception as e: print(url, e, str(time.ctime())) if __name__ == &apos;__main__&apos;: url = &quot;http://example.com&quot; get_data(url) 至此，我们完成了一个网页的抓取，并且做了相关的异常处理。 0x02 解析网页本节说明下，通过 re 和 beautifulsoup 的组合来解析网页 ####1. re (正则表达式) 提取网页经常用的是贪婪匹配和非贪婪匹配，还有就是单行匹配还是全文匹配最常用的方法是 re.findall 还有括号的使用例如 import re text = &quot;&quot;&quot;Sxchaoinfo@EgithubE xchaoinfo@wechat xchaoinfo@zhihuE&quot;&quot;&quot; 模式一 # 加上 re.S 代表是全文匹配 pa = r&apos;S.*?E&apos; re.findall(pa, text) -&gt; 返回的是[&quot;Sxchaoinfo@E&quot;,] re.findall(pa, text, re.S) -&gt; 返回的是[&quot;Sxchaoinfo@E&quot;,] 模式二 pa = r&apos;S.*E&apos; re.findall(pa, text) -&gt; 返回的是[&apos;Sxchaoinfo@EgithubE&apos;, ] re.findall(pa, text, re.S) -&gt; 返回的是 [&apos;Sxchaoinfo@EgithubE\\n xchaoinfo@wechat\\n xchaoinfo@zhihuE&apos;, ] 模式三 pa = r&apos;S(.*?)E&apos; re.findall(pa, text) -&gt; 返回的是[&apos;xchaoinfo@&apos;,] 更佳详细的 re(正则表达式) 的使用方法，请参考文档获取其他网站的教程 ####2. beautifulsoup 提取网页 Beautifulsoup 解析网页的时候推荐使用 lxml作为解析器，虽然 lxml 的安装可能会花费您一点时间，其他解析器的选择，可以参考官方的文档最常用的方法是 from bs4 import BeautifulSoup # 获取页面所有的 url 链接 soup = BeautifulSoup(html_content, &apos;lxml&apos;) ahref = soup.findAll(&apos;a&apos;) for a in ahref: print(a[&apos;href&apos;]) 参考文档 Requests: HTTP for Humans requests 中文文档 BeautifulSoup Documentation BeautifulSoup 中文文档","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]}]